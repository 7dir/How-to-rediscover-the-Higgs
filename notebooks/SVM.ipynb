{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to rediscover the Higgs boson yourself! - SVM\n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to rediscover the Higgs boson yourself!\n",
    "\n",
    "The idea is that you add extra cuts to increase the ratio of signal ($H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$) to background ($Z, t\\bar{t}, ZZ \\rightarrow \\ell\\ell\\ell\\ell$)\n",
    "\n",
    "First, try to reduce the amount of $Z$ and $t\\bar{t}$ background, since these are quite different to the signal.\n",
    "\n",
    "Then, try to reduce the amount of $ZZ \\rightarrow \\ell\\ell\\ell\\ell$, whilst keeping $H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$ signal\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event, so that processing is quicker.\n",
    "\n",
    "This notebook was inspired by https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../HZZ_feynman.pdf\" style=\"width:40%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First time setup\n",
    "This first cell only needs to be run the first time you open this notebook on your computer. \n",
    "\n",
    "If you close jupyter and re-open on the same computer, you won't need to run this first cell again.\n",
    "\n",
    "If you re-open on binder, you will need to run this cell again.\n",
    "\n",
    "If you run into a problem of \"uproot not being available\", Kernel -> Restart & Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade --user pip\n",
    "!{sys.executable} -m pip install -U numpy pandas uproot matplotlib keras scikit-learn --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To setup everytime\n",
    "Cell -> Run All Below\n",
    "\n",
    "to be done every time you re-open this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import infofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 1000\n",
    "                                                                                                                                  \n",
    "tuple_path = \"Input/\"\n",
    "\n",
    "stack_order = ['data',r'$Z,t\\bar{t}$','ZZ',r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['DataEgamma','DataMuons']\n",
    "    },\n",
    "\n",
    "    r'$Z,t\\bar{t}$' : {\n",
    "        'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "        'color' : \"#8700da\"\n",
    "    },\n",
    "\n",
    "    'ZZ' : {\n",
    "        'list' : ['ZZ'],\n",
    "        'color' : \"#f90000\"\n",
    "    },\n",
    "\n",
    "    r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' : {\n",
    "        'list' : ['ggH125_ZZ4lep','VBFH125_ZZ4lep'],\n",
    "        'color' : \"#4faeff\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for s in samples:\n",
    "        print(s+':')\n",
    "        frames = []\n",
    "        for val in samples[s]['list']:\n",
    "            prefix = \"MC/skim.mc_\"\n",
    "            if s == 'data':\n",
    "                prefix = \"Data/skim.\"\n",
    "            else: prefix += str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "            fileString = tuple_path+prefix+val+\".root\"\n",
    "            print(fileString)\n",
    "            f = glob.glob(fileString,recursive=False)[0]\n",
    "            if f != \"\":\n",
    "                temp = read_file(f,val)\n",
    "                frames.append(temp)\n",
    "            else:\n",
    "                print(\"Error: \"+val+\" not found!\")\n",
    "        data[s] = pd.concat(frames)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mllll_window(mllll):\n",
    "    return 120 < mllll < 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(mcWeight,scaleFactor_PILEUP,scaleFactor_ELE,\n",
    "                scaleFactor_MUON, scaleFactor_TRIGGER):\n",
    "    return mcWeight*scaleFactor_PILEUP*scaleFactor_ELE*scaleFactor_MUON*scaleFactor_TRIGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(totalWeight,sample):\n",
    "    info = infofile.infos[sample]\n",
    "    weight = (lumi*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"])\n",
    "    weight *= totalWeight\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations(data, title, **kwds):\n",
    "    \"\"\"Calculate pairwise correlation between features.\n",
    "    \n",
    "    Extra arguments are passed on to DataFrame.corr()\n",
    "    \"\"\"\n",
    "    # simply call df.corr() to get a table of\n",
    "    # correlation values if you do not need\n",
    "    # the fancy plotting\n",
    "    corrmat = data.corr(**kwds)\n",
    "\n",
    "    fig, ax1 = plt.subplots(ncols=1, figsize=(6,5))\n",
    "    \n",
    "    opts = {'cmap': plt.get_cmap(\"RdBu\"),\n",
    "            'vmin': -1, 'vmax': +1}\n",
    "    heatmap1 = ax1.pcolor(corrmat, **opts)\n",
    "    plt.colorbar(heatmap1, ax=ax1)\n",
    "\n",
    "    ax1.set_title(title+\" Correlations\")\n",
    "\n",
    "    labels = corrmat.columns.values\n",
    "    for ax in (ax1,):\n",
    "        # shift location of ticks to center of the bins\n",
    "        ax.set_xticks(np.arange(len(labels))+0.5, minor=False)\n",
    "        ax.set_yticks(np.arange(len(labels))+0.5, minor=False)\n",
    "        ax.set_xticklabels(labels, minor=False, ha='right', rotation=70)\n",
    "        ax.set_yticklabels(labels, minor=False)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "\n",
    "    bins = [80 + x*5 for x in range(35) ]\n",
    "    data_x = [82.5 + x*5 for x in range(34) ]\n",
    "\n",
    "    data_mllll = []\n",
    "    data_mllll_errors = []\n",
    "\n",
    "    mc_mllll = []\n",
    "    mc_weights = []\n",
    "    mc_colors = []\n",
    "    mc_labels = []\n",
    "    mc_in_mllll_window = [] # list for numbers of MC events with 120 < mllll < 130 GeV\n",
    "\n",
    "    for s in stack_order:\n",
    "        if s == \"data\":\n",
    "            data_mllll,_ = np.histogram(data[s].mllll.values, bins=bins)\n",
    "            data_mllll_errors = np.sqrt(data_mllll)\n",
    "        else:\n",
    "            mc_labels.append(s)\n",
    "            mc_mllll.append(data[s].mllll.values)\n",
    "            mc_colors.append(samples[s]['color'])\n",
    "            mc_weights.append(data[s].totalWeight.values)\n",
    "            mc_in_mllll_window.append([data[s].totalWeight.values[mllll_iter] for mllll_iter in range(len(data[s].mllll.values)) if 120 < data[s].mllll.values[mllll_iter] < 130])\n",
    "    \n",
    "    HZZ_in_mllll_window = sum(mc_in_mllll_window[2]) # number signal MC events with 120 < mllll < 130 GeV\n",
    "    background_in_mllll_window = sum(mc_in_mllll_window[0]+mc_in_mllll_window[1]) # number background MC events with 120 < mllll < 130 GeV\n",
    "    SoversqrtB = HZZ_in_mllll_window/math.sqrt(background_in_mllll_window) # calculate significance\n",
    "    print('Signal/sqrt(Background) for 120<mllll<130 '+str(SoversqrtB))\n",
    "    \n",
    "    top = np.amax(data_mllll)+math.sqrt(np.amax(data_mllll))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(mc_mllll,bins=bins,weights=mc_weights,stacked=True,color=mc_colors, label=mc_labels)\n",
    "    plt.errorbar( x=data_x, y=data_mllll, yerr=data_mllll_errors, fmt='ko', label='Data')\n",
    "\n",
    "    #X = np.arange(115,135,5) # gives list [115,120,125,130]\n",
    "    #data_in_window = data_mllll[7:11] # gives list of data y value for [115,120,125,130]\n",
    "    #x = np.sum(X*data_in_window)/np.sum(data_in_window) # Gaussian mean\n",
    "    #width = np.sqrt(np.abs(np.sum((X-x)**2*data_in_window)/np.sum(data_in_window))) # Gaussian width\n",
    "    #fit = lambda t : np.amax(data_in_window)*np.exp(-(t-x)**2/(2*width**2)) # Gaussian fit\n",
    "    #plt.plot(X, fit(X), '-') # plot Gaussian\n",
    "    \n",
    "    plt.xlabel(r'$M_{\\ell\\ell\\ell\\ell}$ [GeV]',fontname='sans-serif',horizontalalignment='right',x=1.0,fontsize=11)\n",
    "\n",
    "    plt.ylabel(r'Events',fontname='sans-serif',horizontalalignment='right',y=1.0,fontsize=11)\n",
    "    #plt.yscale('log')                                                                                                                                                                        \n",
    "    plt.ylim(bottom=0,top=top)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    plt.text(0.05,0.97,r'$\\mathbf{{ATLAS}}$ Open Data',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes,fontsize=13)\n",
    "    plt.text(0.05,0.92,'for education only',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes,style='italic',fontsize=8)\n",
    "    plt.text(0.05,0.9,r'$\\sqrt{s}=8\\,\\mathrm{TeV},\\;\\int L\\,dt=1\\,\\mathrm{fb}^{-1}$',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(\"plot.pdf\")\n",
    "    \n",
    "    # boxplot of signal and background mllll\n",
    "    #plt.figure()\n",
    "    #for s in stack_order:\n",
    "    #    if s != 'data': data[s]['Process'] = s\n",
    "    #data_all = pd.concat([data[r'$Z,t\\bar{t}$'],data['ZZ'],data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']])\n",
    "    #data_all.boxplot(by='Process',column=[\"mllll\"])\n",
    "    \n",
    "    # plot correlation matrices for signal and background\n",
    "    #correlations(pd.concat([data[r'$Z,t\\bar{t}$'],data['ZZ']]),\"background\")\n",
    "    #correlations(data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'], \"signal\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mllll(lep_pts,lep_etas,lep_phis):\n",
    "    theta_0 = 2*math.atan(math.exp(-lep_etas[0]))\n",
    "    theta_1 = 2*math.atan(math.exp(-lep_etas[1]))\n",
    "    theta_2 = 2*math.atan(math.exp(-lep_etas[2]))\n",
    "    theta_3 = 2*math.atan(math.exp(-lep_etas[3]))\n",
    "    p_0 = lep_pts[0]/math.sin(theta_0)\n",
    "    p_1 = lep_pts[1]/math.sin(theta_1)\n",
    "    p_2 = lep_pts[2]/math.sin(theta_2)\n",
    "    p_3 = lep_pts[3]/math.sin(theta_3)\n",
    "    pz_0 = p_0*math.cos(theta_0)\n",
    "    pz_1 = p_1*math.cos(theta_1)\n",
    "    pz_2 = p_2*math.cos(theta_2)\n",
    "    pz_3 = p_3*math.cos(theta_3)\n",
    "    px_0 = p_0*math.sin(theta_0)*math.cos(lep_phis[0])\n",
    "    px_1 = p_1*math.sin(theta_1)*math.cos(lep_phis[1])\n",
    "    px_2 = p_2*math.sin(theta_2)*math.cos(lep_phis[2])\n",
    "    px_3 = p_3*math.sin(theta_3)*math.cos(lep_phis[3])\n",
    "    py_0 = p_0*math.sin(theta_0)*math.sin(lep_phis[0])\n",
    "    py_1 = p_1*math.sin(theta_1)*math.sin(lep_phis[1])\n",
    "    py_2 = p_2*math.sin(theta_2)*math.sin(lep_phis[2])\n",
    "    py_3 = p_3*math.sin(theta_3)*math.sin(lep_phis[3])\n",
    "    sumpz = pz_0 + pz_1 + pz_2 + pz_3\n",
    "    sumpx = px_0 + px_1 + px_2 + px_3\n",
    "    sumpy = py_0 + py_1 + py_2 + py_3\n",
    "    sumE = p_0 + p_1 + p_2 + p_3\n",
    "    mllll = sumE**2 - sumpz**2 - sumpx**2 - sumpy**2\n",
    "    return math.sqrt(mllll)/1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mll(lep_pts,lep_etas,lep_phis):\n",
    "    # this is only pseudo-code to tell you what to do!\n",
    "    # you need to decide how to find i & j yourself\n",
    "    mll = 2*lep_pts[i]*lep_pts[j!=i]\n",
    "    cosh = math.cosh(lep_etas[i]-lep_etas[j!=i])\n",
    "    cos = math.cos(lep_phis[i]-lep_phis[j!=i])\n",
    "    mll *= ( cosh - cos )\n",
    "    return math.sqrt(mll)/1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncommenting a new cut\n",
    "If you add a cut: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,sample):\n",
    "    start = time.time()\n",
    "    print(\"\\tProcessing: \"+sample)\n",
    "    mc = uproot.open(path)[\"mini\"]\n",
    "    data = mc.pandas.df([\"lep_n\",\"lep_pt\",\"lep_eta\",\"lep_phi\",\"lep_charge\",\"lep_type\",\"lep_etcone20\",\"lep_trackd0pvunbiased\",\"lep_tracksigd0pvunbiased\",\n",
    "                         \"mcWeight\",\"scaleFactor_PILEUP\",\"scaleFactor_ELE\",\"scaleFactor_MUON\", # add more variables here if you make cuts on them\n",
    "                         \"scaleFactor_TRIGGER\"], flatten=False)\n",
    "\n",
    "    nIn = len(data.index)\n",
    "\n",
    "    if 'Data' not in sample:\n",
    "        data['totalWeight'] = np.vectorize(calc_weight)(data.mcWeight,data.scaleFactor_PILEUP,data.scaleFactor_ELE,data.scaleFactor_MUON,data.scaleFactor_TRIGGER)\n",
    "        data['totalWeight'] = np.vectorize(get_xsec_weight)(data.totalWeight,sample)\n",
    "\n",
    "    data.drop([\"mcWeight\",\"scaleFactor_PILEUP\",\"scaleFactor_ELE\",\"scaleFactor_MUON\",\"scaleFactor_TRIGGER\"], axis=1, inplace=True)\n",
    "    \n",
    "    # cut on minimum lepton pt\n",
    "    \n",
    "    # cut on lepton etcone20\n",
    "    \n",
    "    # cut on lepton d0\n",
    "    \n",
    "    # example of adding column that takes the return of the function cut_lep_pt_min\n",
    "    #data['lep_pt_min'] = data.apply(cut_lep_pt_min,axis=1)\n",
    "    \n",
    "    # example of cut on minimum number of leptons passing baseline requirements\n",
    "    #fail = data[ np.vectorize(cut_n_lep_min)(data.lep_pt_min) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on number of leptons\n",
    "    fail = data[ np.vectorize(cut_n_lep)(data.lep_n) ].index\n",
    "    data.drop(fail, inplace=True)\n",
    "\n",
    "    # cut on lepton charge\n",
    "    #fail = data[ np.vectorize(cut_lep_charge)(data.lep_charge) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    #print(data)\n",
    "    \n",
    "    # cut on lepton type\n",
    "    #fail = data[ np.vectorize(cut_lep_type)(data.lep_type) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on lepton pt\n",
    "    #fail = data[ np.vectorize(cut_lep_pt)(data.lep_pt) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on deltaR\n",
    "    #fail = data[ np.vectorize(cut_deltaR)(data.lep_eta,data.lep_phi...\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on minimum opposite-charge-same-type lepton pair invariant mass\n",
    "    #fail = data[ np.vectorize(cut_OCST)(data....\n",
    "\n",
    "    # calculation of Z boson candidate 1 invariant mass\n",
    "    #data['mZ1'] = np.vectorize(calc_mZ1)(data.lep_pt,data.lep_eta,data.lep_phi)\n",
    "    \n",
    "    # cut on mZ1\n",
    "    #fail = data[ np.vectorize(cut_mZ1)(data.mZ1) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # calculation of Z boson candidate 2 invariant mass\n",
    "    #data['mZ2'] = np.vectorize(calc_mZ2)(data....\n",
    "    \n",
    "    # cut on mZ2\n",
    "    #fail = data[ np.vectorize(cut_mZ2)(data.mZ2) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # calculation of 4-lepton invariant mass\n",
    "    data['mllll'] = np.vectorize(calc_mllll)(data.lep_pt,data.lep_eta,data.lep_phi)\n",
    "\n",
    "    mllll_window_list = data[ np.vectorize(mllll_window)(data.mllll) ] # return events with 120 < mllll < 130 GeV\n",
    "    \n",
    "    # example of expanding lep_pt list column into individual columns whilst requiring exactly 4 leptons\n",
    "    # need to change cut_lep_n to require exactly 4 leptons\n",
    "    #data[['lep1_pt','lep2_pt','lep3_pt','lep4_pt']] = pd.DataFrame(data.lep_pt.values.tolist(), index= data.index)\n",
    "\n",
    "    # example of expanding lep_pt list column into individual columns without requiring exactly 4 leptons\n",
    "    # need to do this for columns that you wish to use for fit_BDT\n",
    "    #if max(data.lep_n) < 5: \n",
    "    #    df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt'], index=data.index)\n",
    "    #    df_split['lep5_pt'] = 0\n",
    "    #    df_split['lep6_pt'] = 0\n",
    "    #elif max(data.lep_n) < 6: \n",
    "    #    df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt','lep5_pt'], index=data.index)\n",
    "    #    df_split['lep6_pt'] = 0\n",
    "    #else: df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt','lep5_pt','lep6_pt'], index=data.index)\n",
    "    #df_split.fillna(0, inplace=True)\n",
    "    #data = pd.concat([data, df_split], axis=1)\n",
    "    \n",
    "    #print(data)\n",
    "\n",
    "    nOut = len(data.index)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(\"\\t\\tTime taken: \"+str(elapsed)+\", nIn: \"+str(nIn)+\", nOut: \"+str(nOut))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing an already uncommented cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change a cut: Cell -> Run All Below\n",
    "\n",
    "If you uncomment a cut here, you also need to uncomment the corresponding cut in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut on number of leptons\n",
    "def cut_n_lep(lep_n):\n",
    "    # return when number of leptons is less than 4\n",
    "    return lep_n < 4\n",
    "\n",
    "# cut on lepton charge\n",
    "def cut_lep_charge(lep_charge):\n",
    "    # return when sum of lepton charges is not equal to 0\n",
    "    # exclamation mark (!) means \"not\"\n",
    "    # so != means \"not equal to\"\n",
    "    # first lepton is [0], 2nd lepton is [1] etc\n",
    "    return lep_charge[0] + lep_charge[1] + lep_charge[2] + lep_charge[3] != 0\n",
    "\n",
    "# cut on lepton type\n",
    "def cut_lep_type(lep_type):\n",
    "# for an electron lep_type is 11\n",
    "# for a muon lep_type is 13\n",
    "    sum_lep_type = lep_type[0] + lep_type[1] + lep_type[2] + lep_type[3]\n",
    "    return (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 44) and (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 48) and (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 52)\n",
    "\n",
    "# cut on lepton pt\n",
    "#def cut_lep_pt(lep_pt):\n",
    "# want to throw away events where the 2nd highest pt lepton used has lep_pt[1] < 15000\n",
    "# want to throw away events where the 3rd highest pt lepton used has lep_pt[2] < 10000\n",
    "\n",
    "# cut on minimum opposite-charge-same-type lepton pair invariant mass\n",
    "#def cut_mOCST():\n",
    "# want to throw away events if the invariant mass of any opposite-charge-same-type lepton pair is < 5\n",
    "\n",
    "# cut on invariant mass of Z boson candidate 1\n",
    "#def cut_mZ1(mZ1):\n",
    "# want invariant mass of same-type-opposite-charge lepton pair that's closest to Z mass (91 GeV) to be in range 50 < m < 106 GeV\n",
    "\n",
    "# cut on invariant mass of Z boson candidate 2\n",
    "#def cut_mZ2(mZ2):\n",
    "# want invariant mass of remaining lepton pair that's closest to Z mass (91 GeV) to be in range 17.5 < m < 115 GeV\n",
    "# advanced: vary the lower range monotically from 17.5 at mllll=120 to 50 at mllll=190, and constant above mllll=190\n",
    "\n",
    "# cut on deltaR\n",
    "# want to throw away leptons that are separated from all other leptons by deltaR = math.sqrt(delta(lep_eta)**2 + delta(lep_phi)**2) < 0.2\n",
    "# want to throw away leptons that are separated from other leptons of the same type by deltaR = math.sqrt(delta(lep_eta)**2 + delta(lep_phi)**2) < 0.1\n",
    "\n",
    "# example of returning list where every element passes minimum lep_pt requirement\n",
    "#def cut_lep_pt_min(data):\n",
    "#    return [data.lep_pt[i] for i in range(len(data.lep_pt)) if data.lep_pt[i] > 6000]\n",
    "\n",
    "# cut on minimum lepton pt\n",
    "# want to throw away muons with lep_pt < 6000\n",
    "# want to throw away electrons with lep_pt < 7000\n",
    "\n",
    "# cut on maximum lepton etcone20\n",
    "# want to throw away muons with lep_etcone20/lep_pt < 0.3\n",
    "# want to throw away electrons with lep_etcone20/lep_pt < 0.2\n",
    "\n",
    "# cut on maximum lepton d0\n",
    "# want to throw away muons with lep_trackd0pvunbiased/lep_tracksigd0pvunbiased < 3.5\n",
    "# want to throw away electrons with lep_trackd0pvunbiased/lep_tracksigd0pvunbiased < 6.5\n",
    "\n",
    "# example of cutting on length of list passing minimum requirements\n",
    "#def cut_n_lep_min(lep_pt_min):\n",
    "#    return len(lep_pt_min) < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    start = time.time()\n",
    "    data = get_data_from_files()\n",
    "    plot_data(data)\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Time taken: \"+str(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Support Vector Machines\n",
    "\n",
    "Here we will consider discriminative classification: rather than modeling each class, we simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.\n",
    "\n",
    "As an example of this, consider the simple case of a classification task, in which the two classes of points are well separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(data):    \n",
    "    \n",
    "    # scatter plot of signal and background lep_n vs mllll\n",
    "    #plt.figure()\n",
    "    #for s in stack_order:\n",
    "    #    if s != 'data': \n",
    "    #        idx = np.random.choice(np.arange(len(data[s].lep_n)), 100)\n",
    "    #        plt.scatter(data[s].lep_n[idx],data[s].mllll[idx],color=samples[s]['color'],label=s)\n",
    "    #plt.xlabel(r'Leptons',fontname='sans-serif',horizontalalignment='right',y=1.0,fontsize=11)\n",
    "    #plt.ylabel(r'$M_{\\ell\\ell\\ell\\ell}$ [GeV]',fontname='sans-serif',horizontalalignment='right',x=1.0,fontsize=11)\n",
    "    #plt.legend()\n",
    "    \n",
    "    # scatter plot of signal and background lep_pt[0] vs mllll\n",
    "    plt.figure()\n",
    "    for s in stack_order:\n",
    "        if s != 'data': \n",
    "            idx = np.random.choice(np.arange(len(data[s].lep_pt.apply(lambda x: x[0]))), 100)\n",
    "            plt.scatter(data[s].lep_pt.apply(lambda x: x[0])[idx],data[s].mllll[idx],color=samples[s]['color'],label=s)\n",
    "    plt.legend()\n",
    "    \n",
    "scatter_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. For two dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\n",
    "\n",
    "We can draw them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xfit = np.linspace(0, 200000)\n",
    "for s in stack_order:\n",
    "    if s == 'ZZ' or s == r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$': \n",
    "        # plot 15 points to avoid overcrowding\n",
    "        plt.scatter(data[s].lep_pt.apply(lambda x: x[0])[1:16],data[s].mllll[1:16],color=samples[s]['color'],label=s)\n",
    "plt.legend()\n",
    "plt.plot([140000], [150], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(0.00, 160), (0.0003, 150), (-0.0003, 170)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(0, 150000);\n",
    "plt.ylim(80, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are three very different separators which, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label! Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to think a bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines: Maximizing the Margin\n",
    "\n",
    "Support vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a margin of some width, up to the nearest point. Here is an example of how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "xfit = np.linspace(0, 200000)\n",
    "for s in stack_order:\n",
    "    if s == 'ZZ' or s == r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$': \n",
    "        # plot 15 points to avoid overcrowding\n",
    "        plt.scatter(data[s].lep_pt.apply(lambda x: x[0])[1:16],data[s].mllll[1:16],color=samples[s]['color'],label=s)\n",
    "plt.legend()\n",
    "plt.plot([190000], [150], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b, d in [(0.00, 160, 30), (0.0002, 150, 29), (-0.0002, 170, 14)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(0, 150000);\n",
    "plt.ylim(80, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a support vector machine\n",
    "\n",
    "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "\n",
    "# add isSignal variable\n",
    "data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']['isSignal'] = np.ones(len(data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'])) \n",
    "data[r'$Z,t\\bar{t}$']['isSignal'] = np.zeros(len(data[r'$Z,t\\bar{t}$']))\n",
    "data['ZZ']['isSignal'] = np.zeros(len(data['ZZ']))\n",
    "\n",
    "df_all = pd.concat([data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'][1:16],data['ZZ'][1:16]])\n",
    "\n",
    "X = np.vstack((df_all.lep_pt.apply(lambda x: x[0]).values,df_all.mllll.values)).T\n",
    "\n",
    "model.fit(X, df_all['isSignal'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stack_order:\n",
    "    if s == 'ZZ' or s == r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$': \n",
    "        # plot 15 points to avoid overcrowding\n",
    "        plt.scatter(data[s].lep_pt.apply(lambda x: x[0])[1:16],data[s].mllll[1:16],color=samples[s]['color'],label=s)\n",
    "plt.legend()\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure. These points are the pivotal elements of this fit, and are known as the support vectors, and give the algorithm its name. In Scikit-Learn, the identity of these points are stored in the *support_vectors_* attribute of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
    "\n",
    "We can see this, for example, if we plot the model learned from the first 27 points and first 30 points of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=15, ax=None):\n",
    "\n",
    "    # add isSignal variable\n",
    "    data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']['isSignal'] = np.ones(len(data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'])) \n",
    "    data[r'$Z,t\\bar{t}$']['isSignal'] = np.zeros(len(data[r'$Z,t\\bar{t}$']))\n",
    "    data['ZZ']['isSignal'] = np.zeros(len(data['ZZ']))\n",
    "    \n",
    "    df_all = pd.concat([data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'][1:16],data['ZZ'][1:16]]).sample(frac=1)\n",
    "\n",
    "    X = np.vstack((df_all.lep_pt.apply(lambda x: x[0]).values,df_all.mllll.values)).T\n",
    "    \n",
    "    y = df_all['isSignal'].values\n",
    "\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(0, 150000)\n",
    "    ax.set_ylim(80, 250)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [27, 30]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel, we see the model and the support vectors for 27 training points. In the right panel, we have increased the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel. This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
