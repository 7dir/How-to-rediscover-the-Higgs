{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to rediscover the Higgs boson yourself! - BDT\n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to rediscover the Higgs boson yourself!\n",
    "\n",
    "The idea is that you improve upon the example BDT to increase the ratio of signal ($H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$) to background ($Z, t\\bar{t}, ZZ \\rightarrow \\ell\\ell\\ell\\ell$)\n",
    "\n",
    "First, try to reduce the amount of $Z$ and $t\\bar{t}$ background, since these are quite different to the signal.\n",
    "\n",
    "Then, try to reduce the amount of $ZZ \\rightarrow \\ell\\ell\\ell\\ell$, whilst keeping $H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$ signal\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event, so that processing is quicker.\n",
    "\n",
    "This notebook was inspired by https://betatim.github.io/posts/sklearn-for-TMVA-users/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../HZZ_feynman.pdf\" style=\"width:40%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First time setup\n",
    "This first cell only needs to be run the first time you open this notebook on your computer. \n",
    "\n",
    "If you close jupyter and re-open on the same computer, you won't need to run this first cell again.\n",
    "\n",
    "If you re-open on binder, you will need to run this cell again.\n",
    "\n",
    "If you run into a problem of \"uproot not being available\", Kernel -> Restart & Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade --user pip\n",
    "!{sys.executable} -m pip install -U numpy pandas uproot matplotlib keras scikit-learn --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To setup everytime\n",
    "Cell -> Run All Below\n",
    "\n",
    "to be done every time you re-open this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import infofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 1000\n",
    "                                                                                                                                  \n",
    "tuple_path = \"Input/\"\n",
    "\n",
    "stack_order = ['data',r'$Z,t\\bar{t}$','ZZ',r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['DataEgamma','DataMuons']\n",
    "    },\n",
    "\n",
    "    r'$Z,t\\bar{t}$' : {\n",
    "        'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "        'color' : \"#8700da\"\n",
    "    },\n",
    "\n",
    "    'ZZ' : {\n",
    "        'list' : ['ZZ'],\n",
    "        'color' : \"#f90000\"\n",
    "    },\n",
    "\n",
    "    r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' : {\n",
    "        'list' : ['ggH125_ZZ4lep','VBFH125_ZZ4lep'],\n",
    "        'color' : \"#4faeff\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for s in samples:\n",
    "        print(s+':')\n",
    "        frames = []\n",
    "        for val in samples[s]['list']:\n",
    "            prefix = \"MC/skim.mc_\"\n",
    "            if s == 'data':\n",
    "                prefix = \"Data/skim.\"\n",
    "            else: prefix += str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "            fileString = tuple_path+prefix+val+\".root\"\n",
    "            print(fileString)\n",
    "            f = glob.glob(fileString,recursive=False)[0]\n",
    "            if f != \"\":\n",
    "                temp = read_file(f,val)\n",
    "                frames.append(temp)\n",
    "            else:\n",
    "                print(\"Error: \"+val+\" not found!\")\n",
    "        data[s] = pd.concat(frames)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mllll_window(mllll):\n",
    "    return 120 < mllll < 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(mcWeight,scaleFactor_PILEUP,scaleFactor_ELE,\n",
    "                scaleFactor_MUON, scaleFactor_TRIGGER):\n",
    "    return mcWeight*scaleFactor_PILEUP*scaleFactor_ELE*scaleFactor_MUON*scaleFactor_TRIGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(totalWeight,sample):\n",
    "    info = infofile.infos[sample]\n",
    "    weight = (lumi*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"])\n",
    "    weight *= totalWeight\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Variables and Correlations\n",
    "\n",
    "The <span style=\"color:red\">pandas</span> library provides high-performance, easy-to-use data structures and data analysis tools written in python. We will use it for its plotting capabilities and other nice ways to explore your data.\n",
    "\n",
    "A <span style=\"color:red\">DataFrame</span> is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "\n",
    "    mc_weights = []\n",
    "    mc_colors = []\n",
    "    mc_labels = []\n",
    "\n",
    "    for s in stack_order:\n",
    "        if s != \"data\":\n",
    "            mc_labels.append(s)\n",
    "            mc_colors.append(samples[s]['color'])\n",
    "    \n",
    "    # scatter plot of signal and background lep_n vs mllll\n",
    "    # need to remove lep_n from drop at end of read_file function for this to work\n",
    "    #plt.figure()\n",
    "    #for s in stack_order:\n",
    "    #    if s != 'data': plt.scatter(data[s].lep_n,data[s].mllll,color=samples[s]['color'],label=s)\n",
    "    #plt.xlabel(r'Leptons',fontname='sans-serif',horizontalalignment='right',y=1.0,fontsize=11)\n",
    "    #plt.ylabel(r'$M_{\\ell\\ell\\ell\\ell}$ [GeV]',fontname='sans-serif',horizontalalignment='right',x=1.0,fontsize=11)\n",
    "    #plt.legend()\n",
    "    \n",
    "    # scatter plot of signal and background lep_pt[0] vs mllll\n",
    "    # need to remove lep_pt from drop at end of read_file function for this to work\n",
    "    #plt.figure()\n",
    "    #for s in stack_order:\n",
    "    #    if s != 'data': plt.scatter(data[s].lep_pt.apply(lambda x: x[0]),data[s].mllll,color=samples[s]['color'],label=s)\n",
    "    #plt.legend()\n",
    "    \n",
    "    # plot correlation matrices for signal and background\n",
    "    # remove variables from drop at end of read_file function to see their correlations\n",
    "    #correlations(pd.concat([data[r'$Z,t\\bar{t}$'],data['ZZ']]),\"background\")\n",
    "    #correlations(data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'], \"signal\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix\n",
    "The correlation between features can be plotted with <span style=\"color:red\">df.corr()</span>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations(data, title, **kwds):\n",
    "    \"\"\"Calculate pairwise correlation between features.\n",
    "    \n",
    "    Extra arguments are passed on to DataFrame.corr()\n",
    "    \"\"\"\n",
    "    # simply call df.corr() to get a table of\n",
    "    # correlation values if you do not need\n",
    "    # the fancy plotting\n",
    "    corrmat = data.corr(**kwds)\n",
    "\n",
    "    fig, ax1 = plt.subplots(ncols=1, figsize=(6,5))\n",
    "    \n",
    "    opts = {'cmap': plt.get_cmap(\"RdBu\"),\n",
    "            'vmin': -1, 'vmax': +1}\n",
    "    heatmap1 = ax1.pcolor(corrmat, **opts)\n",
    "    plt.colorbar(heatmap1, ax=ax1)\n",
    "\n",
    "    ax1.set_title(title+\" Correlations\")\n",
    "\n",
    "    labels = corrmat.columns.values\n",
    "    for ax in (ax1,):\n",
    "        # shift location of ticks to center of the bins\n",
    "        ax.set_xticks(np.arange(len(labels))+0.5, minor=False)\n",
    "        ax.set_yticks(np.arange(len(labels))+0.5, minor=False)\n",
    "        ax.set_xticklabels(labels, minor=False, ha='right', rotation=70)\n",
    "        ax.set_yticklabels(labels, minor=False)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mllll(lep_pts,lep_etas,lep_phis):\n",
    "    theta_0 = 2*math.atan(math.exp(-lep_etas[0]))\n",
    "    theta_1 = 2*math.atan(math.exp(-lep_etas[1]))\n",
    "    theta_2 = 2*math.atan(math.exp(-lep_etas[2]))\n",
    "    theta_3 = 2*math.atan(math.exp(-lep_etas[3]))\n",
    "    p_0 = lep_pts[0]/math.sin(theta_0)\n",
    "    p_1 = lep_pts[1]/math.sin(theta_1)\n",
    "    p_2 = lep_pts[2]/math.sin(theta_2)\n",
    "    p_3 = lep_pts[3]/math.sin(theta_3)\n",
    "    pz_0 = p_0*math.cos(theta_0)\n",
    "    pz_1 = p_1*math.cos(theta_1)\n",
    "    pz_2 = p_2*math.cos(theta_2)\n",
    "    pz_3 = p_3*math.cos(theta_3)\n",
    "    px_0 = p_0*math.sin(theta_0)*math.cos(lep_phis[0])\n",
    "    px_1 = p_1*math.sin(theta_1)*math.cos(lep_phis[1])\n",
    "    px_2 = p_2*math.sin(theta_2)*math.cos(lep_phis[2])\n",
    "    px_3 = p_3*math.sin(theta_3)*math.cos(lep_phis[3])\n",
    "    py_0 = p_0*math.sin(theta_0)*math.sin(lep_phis[0])\n",
    "    py_1 = p_1*math.sin(theta_1)*math.sin(lep_phis[1])\n",
    "    py_2 = p_2*math.sin(theta_2)*math.sin(lep_phis[2])\n",
    "    py_3 = p_3*math.sin(theta_3)*math.sin(lep_phis[3])\n",
    "    sumpz = pz_0 + pz_1 + pz_2 + pz_3\n",
    "    sumpx = px_0 + px_1 + px_2 + px_3\n",
    "    sumpy = py_0 + py_1 + py_2 + py_3\n",
    "    sumE = p_0 + p_1 + p_2 + p_3\n",
    "    mllll = sumE**2 - sumpz**2 - sumpx**2 - sumpy**2\n",
    "    return math.sqrt(mllll)/1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mll(lep_pts,lep_etas,lep_phis):\n",
    "    # this is only pseudo-code to tell you what to do!\n",
    "    # you need to decide how to find i & j yourself\n",
    "    mll = 2*lep_pts[i]*lep_pts[j!=i]\n",
    "    cosh = math.cosh(lep_etas[i]-lep_etas[j!=i])\n",
    "    cos = math.cos(lep_phis[i]-lep_phis[j!=i])\n",
    "    mll *= ( cosh - cos )\n",
    "    return math.sqrt(mll)/1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncommenting a new cut\n",
    "If you add a cut: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,sample):\n",
    "    start = time.time()\n",
    "    print(\"\\tProcessing: \"+sample)\n",
    "    mc = uproot.open(path)[\"mini\"]\n",
    "    data = mc.pandas.df([\"lep_n\",\"lep_pt\",\"lep_eta\",\"lep_phi\",\"lep_charge\",\"lep_type\",\"lep_etcone20\",\"lep_trackd0pvunbiased\",\"lep_tracksigd0pvunbiased\",\n",
    "                         \"mcWeight\",\"scaleFactor_PILEUP\",\"scaleFactor_ELE\",\"scaleFactor_MUON\", # add more variables here if you make cuts on them\n",
    "                         \"scaleFactor_TRIGGER\"], flatten=False)\n",
    "\n",
    "    nIn = len(data.index)\n",
    "\n",
    "    if 'Data' not in sample:\n",
    "        data['totalWeight'] = np.vectorize(calc_weight)(data.mcWeight,data.scaleFactor_PILEUP,data.scaleFactor_ELE,data.scaleFactor_MUON,data.scaleFactor_TRIGGER)\n",
    "        data['totalWeight'] = np.vectorize(get_xsec_weight)(data.totalWeight,sample)\n",
    "\n",
    "    data.drop([\"mcWeight\",\"scaleFactor_PILEUP\",\"scaleFactor_ELE\",\"scaleFactor_MUON\",\"scaleFactor_TRIGGER\"], axis=1, inplace=True)\n",
    "    \n",
    "    # cut on minimum lepton pt\n",
    "    \n",
    "    # cut on lepton etcone20\n",
    "    \n",
    "    # cut on lepton d0\n",
    "    \n",
    "    # example of adding column that takes the return of the function cut_lep_pt_min\n",
    "    #data['lep_pt_min'] = data.apply(cut_lep_pt_min,axis=1)\n",
    "    \n",
    "    # example of cut on minimum number of leptons passing baseline requirements\n",
    "    #fail = data[ np.vectorize(cut_n_lep_min)(data.lep_pt_min) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on number of leptons\n",
    "    fail = data[ np.vectorize(cut_n_lep)(data.lep_n) ].index\n",
    "    data.drop(fail, inplace=True)\n",
    "\n",
    "    # cut on lepton charge\n",
    "    #fail = data[ np.vectorize(cut_lep_charge)(data.lep_charge) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    #print(data)\n",
    "    \n",
    "    # cut on lepton type\n",
    "    #fail = data[ np.vectorize(cut_lep_type)(data.lep_type) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on lepton pt\n",
    "    #fail = data[ np.vectorize(cut_lep_pt)(data.lep_pt) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on deltaR\n",
    "    #fail = data[ np.vectorize(cut_deltaR)(data.lep_eta,data.lep_phi...\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # cut on minimum opposite-charge-same-type lepton pair invariant mass\n",
    "    #fail = data[ np.vectorize(cut_OCST)(data....\n",
    "\n",
    "    # calculation of Z boson candidate 1 invariant mass\n",
    "    #data['mZ1'] = np.vectorize(calc_mZ1)(data.lep_pt,data.lep_eta,data.lep_phi)\n",
    "    \n",
    "    # cut on mZ1\n",
    "    #fail = data[ np.vectorize(cut_mZ1)(data.mZ1) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # calculation of Z boson candidate 2 invariant mass\n",
    "    #data['mZ2'] = np.vectorize(calc_mZ2)(data....\n",
    "    \n",
    "    # cut on mZ2\n",
    "    #fail = data[ np.vectorize(cut_mZ2)(data.mZ2) ].index\n",
    "    #data.drop(fail, inplace=True)\n",
    "    \n",
    "    # calculation of 4-lepton invariant mass\n",
    "    data['mllll'] = np.vectorize(calc_mllll)(data.lep_pt,data.lep_eta,data.lep_phi)\n",
    "\n",
    "    mllll_window_list = data[ np.vectorize(mllll_window)(data.mllll) ] # return events with 120 < mllll < 130 GeV\n",
    "    \n",
    "    # example of expanding lep_pt list column into individual columns whilst requiring exactly 4 leptons\n",
    "    # need to change cut_lep_n to require exactly 4 leptons\n",
    "    #data[['lep1_pt','lep2_pt','lep3_pt','lep4_pt']] = pd.DataFrame(data.lep_pt.values.tolist(), index= data.index)\n",
    "\n",
    "    # example of expanding lep_pt list column into individual columns without requiring exactly 4 leptons\n",
    "    # need to do this for columns that you wish to use for fit_BDT\n",
    "    #if max(data.lep_n) < 5: \n",
    "    #    df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt'], index=data.index)\n",
    "    #    df_split['lep5_pt'] = 0\n",
    "    #    df_split['lep6_pt'] = 0\n",
    "    #elif max(data.lep_n) < 6: \n",
    "    #    df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt','lep5_pt'], index=data.index)\n",
    "    #    df_split['lep6_pt'] = 0\n",
    "    #else: df_split = pd.DataFrame(data['lep_pt'].values.tolist(), columns=['lep1_pt','lep2_pt','lep3_pt','lep4_pt','lep5_pt','lep6_pt'], index=data.index)\n",
    "    #df_split.fillna(0, inplace=True)\n",
    "    #data = pd.concat([data, df_split], axis=1)\n",
    "    \n",
    "    #print(data)\n",
    "    \n",
    "    # if you want a variable to be input into your BDT, remove it from the drop function\n",
    "    data.drop([\"lep_n\",\"lep_pt\",\"lep_eta\",\"lep_phi\",\"lep_charge\",\"lep_type\",\"lep_etcone20\",\"lep_trackd0pvunbiased\",\"lep_tracksigd0pvunbiased\"], axis=1, inplace=True)  \n",
    "    \n",
    "    # totalWeight needs to be dropped if you want to do bdt\n",
    "    # if this is done, need to comment out plot_data(data) in bottom cell\n",
    "    if 'Data' not in sample: data.drop([\"totalWeight\"], axis=1, inplace=True)\n",
    "\n",
    "    nOut = len(data.index)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(\"\\t\\tTime taken: \"+str(elapsed)+\", nIn: \"+str(nIn)+\", nOut: \"+str(nOut))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing an already uncommented cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change a cut: Cell -> Run All Below\n",
    "\n",
    "If you uncomment a cut here, you also need to uncomment the corresponding cut in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut on number of leptons\n",
    "def cut_n_lep(lep_n):\n",
    "    # return when number of leptons is less than 4\n",
    "    return lep_n < 4\n",
    "\n",
    "# cut on lepton charge\n",
    "def cut_lep_charge(lep_charge):\n",
    "    # return when sum of lepton charges is not equal to 0\n",
    "    # exclamation mark (!) means \"not\"\n",
    "    # so != means \"not equal to\"\n",
    "    # first lepton is [0], 2nd lepton is [1] etc\n",
    "    return lep_charge[0] + lep_charge[1] + lep_charge[2] + lep_charge[3] != 0\n",
    "\n",
    "# cut on lepton type\n",
    "def cut_lep_type(lep_type):\n",
    "# for an electron lep_type is 11\n",
    "# for a muon lep_type is 13\n",
    "    sum_lep_type = lep_type[0] + lep_type[1] + lep_type[2] + lep_type[3]\n",
    "    return (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 44) and (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 48) and (lep_type[0]+lep_type[1]+lep_type[2]+lep_type[3] != 52)\n",
    "\n",
    "# cut on lepton pt\n",
    "#def cut_lep_pt(lep_pt):\n",
    "# want to throw away events where the 2nd highest pt lepton used has lep_pt[1] < 15000\n",
    "# want to throw away events where the 3rd highest pt lepton used has lep_pt[2] < 10000\n",
    "\n",
    "# cut on minimum opposite-charge-same-type lepton pair invariant mass\n",
    "#def cut_mOCST():\n",
    "# want to throw away events if the invariant mass of any opposite-charge-same-type lepton pair is < 5\n",
    "\n",
    "# cut on invariant mass of Z boson candidate 1\n",
    "#def cut_mZ1(mZ1):\n",
    "# want invariant mass of same-type-opposite-charge lepton pair that's closest to Z mass (91 GeV) to be in range 50 < m < 106 GeV\n",
    "\n",
    "# cut on invariant mass of Z boson candidate 2\n",
    "#def cut_mZ2(mZ2):\n",
    "# want invariant mass of remaining lepton pair that's closest to Z mass (91 GeV) to be in range 17.5 < m < 115 GeV\n",
    "# advanced: vary the lower range monotically from 17.5 at mllll=120 to 50 at mllll=190, and constant above mllll=190\n",
    "\n",
    "# cut on deltaR\n",
    "# want to throw away leptons that are separated from all other leptons by deltaR = math.sqrt(delta(lep_eta)**2 + delta(lep_phi)**2) < 0.2\n",
    "# want to throw away leptons that are separated from other leptons of the same type by deltaR = math.sqrt(delta(lep_eta)**2 + delta(lep_phi)**2) < 0.1\n",
    "\n",
    "# example of returning list where every element passes minimum lep_pt requirement\n",
    "#def cut_lep_pt_min(data):\n",
    "#    return [data.lep_pt[i] for i in range(len(data.lep_pt)) if data.lep_pt[i] > 6000]\n",
    "\n",
    "# cut on minimum lepton pt\n",
    "# want to throw away muons with lep_pt < 6000\n",
    "# want to throw away electrons with lep_pt < 7000\n",
    "\n",
    "# cut on maximum lepton etcone20\n",
    "# want to throw away muons with lep_etcone20/lep_pt < 0.3\n",
    "# want to throw away electrons with lep_etcone20/lep_pt < 0.2\n",
    "\n",
    "# cut on maximum lepton d0\n",
    "# want to throw away muons with lep_trackd0pvunbiased/lep_tracksigd0pvunbiased < 3.5\n",
    "# want to throw away electrons with lep_trackd0pvunbiased/lep_tracksigd0pvunbiased < 6.5\n",
    "\n",
    "# example of cutting on length of list passing minimum requirements\n",
    "#def cut_n_lep_min(lep_pt_min):\n",
    "#    return len(lep_pt_min) < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    start = time.time()\n",
    "    data = get_data_from_files()\n",
    "    #plot_data(data)\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Time taken: \"+str(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sklearn data is usually organised\n",
    "# into one 2D array of shape (n_samples x n_features)\n",
    "# containing all the data and one array of categories\n",
    "# of length n_samples\n",
    "X = np.concatenate((data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'], data[r'$Z,t\\bar{t}$'],data['ZZ']))\n",
    "y = np.concatenate((np.ones(data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'].shape[0]),\n",
    "                    np.zeros(data[r'$Z,t\\bar{t}$'].shape[0]),\n",
    "                    np.zeros(data['ZZ'].shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots\n",
    "\n",
    "A Box plot is a nice way of visualising a distribution by showing it's quantiles. Below we make box plots for the feature <span style=\"color:red\">mllll</span> using <span style=\"color:red\">df.boxplot()</span>. The plot shows the distribution of mllll for signal and background. You can also group the distributions by the any other feature in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(data):\n",
    "    \n",
    "    # boxplot of signal and background mllll\n",
    "    plt.figure()\n",
    "    for s in stack_order:\n",
    "        if s != 'data': data[s]['Process'] = s\n",
    "    data_all = pd.concat([data[r'$Z,t\\bar{t}$'],data['ZZ'],data[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']])\n",
    "    data_all.boxplot(by='Process',column=[\"mllll\"])\n",
    "\n",
    "    return\n",
    "\n",
    "boxplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training and Testing split\n",
    "One of the first things to do is split your data into a training and testing set. This will split your data into train-test sets: 67%-33%. It will also shuffle entries so you will not get the first 67% of <span style=\"color:red\">X</span> for training and the last 33% for testing. This is particularly important in cases where you load all signal events first and then the background events.\n",
    "\n",
    "Here we split our data into two independent samples twice. The first split is to create a development and a evaluation set. All development and performance evaluation will be done with the development set. Once the hyper-parameters and other settings are frozen we can use the evaluation set to get an unbiased estimate of the performance.\n",
    "\n",
    "The development set is further split into a training and testing set. The first will be used for training the classifier and the second to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_dev,X_eval, y_dev,y_eval = train_test_split(X, y,\n",
    "                                                test_size=0.33, random_state=42)\n",
    "X_train,X_test, y_train,y_test = train_test_split(X_dev, y_dev,\n",
    "                                                    test_size=0.33, random_state=492)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Decision Trees\n",
    "Here we set several hyper-parameters to non default values.\n",
    "\n",
    "After instantiating our <span style=\"color:red\">AdaBoostClassifier</span>, call the <span style=\"color:red\">fit()</span> method with the training sample as an argument. This will train the tree, now we are ready to evaluate the performance on the held out testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "                            #min_samples_leaf=0.05*len(X_train)\n",
    "\n",
    "#dt.fit(X_train, y_train)\n",
    "\n",
    "bdt = AdaBoostClassifier(dt,\n",
    "                        algorithm='SAMME',\n",
    "                        n_estimators=800,\n",
    "                        learning_rate=0.5)\n",
    "\n",
    "bdt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <span style=\"color:red\">fit()</span> method returns the trained classifier. When printed out all the hyper-parameters are listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing a Classifier's Performance\n",
    "Next let's create a quick report on how well our classifier is doing. It is important to make sure you use samples not seen by the classifier to get an unbiased estimate of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = bdt.predict(X_test)\n",
    "print (classification_report(y_test, y_predicted,\n",
    "                            target_names=[\"background\", \"signal\"]))\n",
    "print (\"Area under ROC curve for test data: %.4f\"%(roc_auc_score(y_test,\n",
    "                                                    bdt.decision_function(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate that point, here's the same performance metrics evaluated on the training set instead. The estimates of the performance are more optimistic than on an unseen set of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = bdt.predict(X_train)\n",
    "print (classification_report(y_train, y_predicted,\n",
    "                            target_names=[\"background\", \"signal\"]))\n",
    "print (\"Area under ROC curve for training data: %.4f\"%(roc_auc_score(y_train,\n",
    "                                                    bdt.decision_function(X_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful plot to judge the performance of a classifier is to look at the ROC curve directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "decisions = bdt.decision_function(X_test)\n",
    "# Compute ROC curve and area under the curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, decisions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC (area = %0.2f)'%(roc_auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overtraining Check\n",
    "Comparing the BDT's output distribution for the training and testing set is a popular way in HEP to check for overtraining. The <span style=\"color:red\">compare_test_train()</span> method will plot the BDT's decision function for each class, as well as overlaying it with the shape of the decision function in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_train_test(clf, X_train, y_train, X_test, y_test, bins=30):\n",
    "    decisions = []\n",
    "    for X,y in ((X_train, y_train), (X_test, y_test)):\n",
    "        d1 = clf.decision_function(X[y>0.5]).ravel()\n",
    "        d2 = clf.decision_function(X[y<0.5]).ravel()\n",
    "        decisions += [d1, d2]\n",
    "        \n",
    "    low = min(np.min(d) for d in decisions)\n",
    "    high = max(np.max(d) for d in decisions)\n",
    "    low_high = (low,high)\n",
    "    \n",
    "    plt.hist(decisions[0],\n",
    "             color='r', alpha=0.5, range=low_high, bins=bins,\n",
    "             histtype='stepfilled', density=True,\n",
    "             label='S (train)')\n",
    "    plt.hist(decisions[1],\n",
    "             color='b', alpha=0.5, range=low_high, bins=bins,\n",
    "             histtype='stepfilled', density=True,\n",
    "             label='B (train)')\n",
    "\n",
    "    hist, bins = np.histogram(decisions[2],\n",
    "                              bins=bins, range=low_high, density=True)\n",
    "    scale = len(decisions[2]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "    \n",
    "    width = (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='S (test)')\n",
    "    \n",
    "    hist, bins = np.histogram(decisions[3],\n",
    "                              bins=bins, range=low_high, density=True)\n",
    "    scale = len(decisions[2]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='B (test)')\n",
    "\n",
    "    plt.xlabel(\"BDT output\")\n",
    "    plt.ylabel(\"Arbitrary units\")\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "compare_train_test(bdt, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
