{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to rediscover the Higgs boson yourself!\n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to rediscover the Higgs boson yourself!\n",
    "\n",
    "The idea is that you add extra cuts to increase the ratio of signal ($H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$) to background ($Z, t\\bar{t}, ZZ \\rightarrow \\ell\\ell\\ell\\ell$)\n",
    "\n",
    "First, try to reduce the amount of $Z$ and $t\\bar{t}$ background, since these are quite different to the signal.\n",
    "\n",
    "Then, try to reduce the amount of $ZZ \\rightarrow \\ell\\ell\\ell\\ell$, whilst keeping $H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$ signal\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event, so that processing is quicker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"HZZ_feynman.pdf\" style=\"width:40%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First time setup\n",
    "This first cell only needs to be run the first time you ever open this notebook. \n",
    "\n",
    "If you close jupyter and re-open on the same computer, you won't need to run this first cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade --user pip\n",
    "!{sys.executable} -m pip install -U numpy pandas uproot matplotlib --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To setup everytime\n",
    "Cell -> Run All Below\n",
    "\n",
    "to be done every time you re-open this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import infofile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 1000\n",
    "                                                                                                                                  \n",
    "tuple_path = \"Input/\"\n",
    "\n",
    "stack_order = ['data',r'$Z,t\\bar{t}$','ZZ',r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['DataEgamma','DataMuons']\n",
    "    },\n",
    "\n",
    "    r'$Z,t\\bar{t}$' : {\n",
    "        'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "        'color' : \"#8700da\"\n",
    "    },\n",
    "\n",
    "    'ZZ' : {\n",
    "        'list' : ['ZZ'],\n",
    "        'color' : \"#f90000\"\n",
    "    },\n",
    "\n",
    "    r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' : {\n",
    "        'list' : ['ggH125_ZZ4lep','VBFH125_ZZ4lep'],\n",
    "        'color' : \"#4faeff\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for s in samples:\n",
    "        print(s+':')\n",
    "        frames = []\n",
    "        for val in samples[s]['list']:\n",
    "            prefix = \"MC/skim.mc_\"\n",
    "            if s == 'data':\n",
    "                prefix = \"Data/skim.\"\n",
    "            else: prefix += str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "            fileString = tuple_path+prefix+val+\".root\"\n",
    "            print(fileString)\n",
    "            f = glob.glob(fileString,recursive=False)[0]\n",
    "            if f != \"\":\n",
    "                temp = read_file(f,val)\n",
    "                frames.append(temp)\n",
    "            else:\n",
    "                print(\"Error: \"+val+\" not found!\")\n",
    "        data[s] = pd.concat(frames)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mllll(lep_pts,lep_etas,lep_phis):\n",
    "    theta_0 = 2*math.atan(math.exp(-lep_etas[0]))\n",
    "    theta_1 = 2*math.atan(math.exp(-lep_etas[1]))\n",
    "    theta_2 = 2*math.atan(math.exp(-lep_etas[2]))\n",
    "    theta_3 = 2*math.atan(math.exp(-lep_etas[3]))\n",
    "    p_0 = lep_pts[0]/math.sin(theta_0)\n",
    "    p_1 = lep_pts[1]/math.sin(theta_1)\n",
    "    p_2 = lep_pts[2]/math.sin(theta_2)\n",
    "    p_3 = lep_pts[3]/math.sin(theta_3)\n",
    "    pz_0 = p_0*math.cos(theta_0)\n",
    "    pz_1 = p_1*math.cos(theta_1)\n",
    "    pz_2 = p_2*math.cos(theta_2)\n",
    "    pz_3 = p_3*math.cos(theta_3)\n",
    "    px_0 = p_0*math.sin(theta_0)*math.cos(lep_phis[0])\n",
    "    px_1 = p_1*math.sin(theta_1)*math.cos(lep_phis[1])\n",
    "    px_2 = p_2*math.sin(theta_2)*math.cos(lep_phis[2])\n",
    "    px_3 = p_3*math.sin(theta_3)*math.cos(lep_phis[3])\n",
    "    py_0 = p_0*math.sin(theta_0)*math.sin(lep_phis[0])\n",
    "    py_1 = p_1*math.sin(theta_1)*math.sin(lep_phis[1])\n",
    "    py_2 = p_2*math.sin(theta_2)*math.sin(lep_phis[2])\n",
    "    py_3 = p_3*math.sin(theta_3)*math.sin(lep_phis[3])\n",
    "    sumpz = pz_0 + pz_1 + pz_2 + pz_3\n",
    "    sumpx = px_0 + px_1 + px_2 + px_3\n",
    "    sumpy = py_0 + py_1 + py_2 + py_3\n",
    "    sumE = p_0 + p_1 + p_2 + p_3\n",
    "    mllll = sumE**2 - sumpz**2 - sumpx**2 - sumpy**2\n",
    "    return math.sqrt(mllll)/1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(mcWeight,scaleFactor_PILEUP,scaleFactor_ELE,\n",
    "                scaleFactor_MUON, scaleFactor_TRIGGER):\n",
    "    return mcWeight*scaleFactor_PILEUP*scaleFactor_ELE*scaleFactor_MUON*scaleFactor_TRIGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(totalWeight,sample):\n",
    "    info = infofile.infos[sample]\n",
    "    weight = (lumi*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"])\n",
    "    weight *= totalWeight\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "\n",
    "    bins = [80 + x*5 for x in range(35) ]\n",
    "    data_x = [82.5 + x*5 for x in range(34) ]\n",
    "\n",
    "    data_mllll = []\n",
    "    data_mllll_errors = []\n",
    "\n",
    "    mc_mllll = []\n",
    "    mc_weights = []\n",
    "    mc_colors = []\n",
    "    mc_labels = []\n",
    "\n",
    "    for s in stack_order:\n",
    "        if s == \"data\":\n",
    "            data_mllll,_ = np.histogram(data[s].mllll.values, bins=bins)\n",
    "            data_mllll_errors = np.sqrt(data_mllll)\n",
    "        else:\n",
    "            mc_labels.append(s)\n",
    "            mc_mllll.append(data[s].mllll.values)\n",
    "            mc_colors.append(samples[s]['color'])\n",
    "            mc_weights.append(data[s].totalWeight.values)\n",
    "\n",
    "    top = np.amax(data_mllll)+math.sqrt(np.amax(data_mllll))\n",
    "\n",
    "    plt.hist(mc_mllll,bins=bins,weights=mc_weights,stacked=True,color=mc_colors, label=mc_labels)\n",
    "    plt.errorbar( x=data_x, y=data_mllll, yerr=data_mllll_errors, fmt='ko', label='Data')\n",
    "\n",
    "    plt.xlabel(r'$M_{\\ell\\ell\\ell\\ell}$ [GeV]',fontname='sans-serif',horizontalalignment='right',x=1.0,fontsize=11)\n",
    "\n",
    "    plt.ylabel(r'Events',fontname='sans-serif',horizontalalignment='right',y=1.0,fontsize=11)\n",
    "    #plt.yscale('log')                                                                                                                                                                        \n",
    "    plt.ylim(bottom=0,top=top)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    plt.text(0.05,0.97,r'$\\mathbf{{ATLAS}}$ Open Data',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes,fontsize=13)\n",
    "    plt.text(0.05,0.92,'for education only',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes,style='italic',fontsize=8)\n",
    "    plt.text(0.05,0.9,r'$\\sqrt{s}=8\\,\\mathrm{TeV},\\;\\int L\\,dt=1\\,\\mathrm{fb}^{-1}$',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    #plt.savefig(\"plot.pdf\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a cut\n",
    "If you add a cut: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,sample):\n",
    "    start = time.time()\n",
    "    print(\"\\tProcessing: \"+sample)\n",
    "    mc = uproot.open(path)[\"mini\"]\n",
    "    data = mc.pandas.df([\"lep_n\",\"lep_pt\",\"lep_eta\",\"lep_phi\",\"lep_E\",\"lep_type\",\"lep_charge\",\"lep_etcone20\",\"lep_trackd0pvunbiased\",\"lep_tracksigd0pvunbiased\",\n",
    "                         \"mcWeight\",\"scaleFactor_PILEUP\",\"scaleFactor_ELE\",\"scaleFactor_MUON\",\n",
    "                         \"scaleFactor_TRIGGER\"], flatten=False)\n",
    "\n",
    "    nIn = len(data.index)\n",
    "\n",
    "    if 'Data' not in sample:\n",
    "        data['totalWeight'] = np.vectorize(calc_weight)(data.mcWeight,data.scaleFactor_PILEUP,data.scaleFactor_ELE,data.scaleFactor_MUON,data.scaleFactor_TRIGGER)\n",
    "        data['totalWeight'] = np.vectorize(get_xsec_weight)(data.totalWeight,sample)\n",
    "\n",
    "    data.drop([\"mcWeight\",\"scaleFactor_PILEUP\",\"scaleFactor_ELE\",\"scaleFactor_MUON\",\"scaleFactor_TRIGGER\"], axis=1, inplace=True)\n",
    "\n",
    "    fail = data[ np.vectorize(cut_n_lep)(data.lep_n) ].index\n",
    "    data.drop(fail, inplace=True)\n",
    "\n",
    "    data['mllll'] = np.vectorize(calc_mllll)(data.lep_pt,data.lep_eta,data.lep_phi)\n",
    "\n",
    "    #print(data)                                                                                                                                                                              \n",
    "\n",
    "    nOut = len(data.index)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(\"\\t\\tTime taken: \"+str(elapsed)+\", nIn: \"+str(nIn)+\", nOut: \"+str(nOut))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing a cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you change a cut: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_n_lep(lep_n):\n",
    "    return lep_n < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    start = time.time()\n",
    "    data = get_data_from_files()\n",
    "    plot_data(data)\n",
    "    elapsed = time.time() - start\n",
    "    print(\"Time taken: \"+str(elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
